{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f49c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "random.seed(1337)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from torchvision import transforms \n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e365e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"image_folder\"  # Replace with your actual path\n",
    "caption_file = \"captions.txt\"        # Your captions file\n",
    "batch_size = 4\n",
    "vocab_size = None  # Will infer from captions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02674c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"Benchmark-AllVideos-HQ-Encoded-challenge/_a6lFCUYTA4.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555c49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames(video_path):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frames = []\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         frames.append(frame)\n",
    "    \n",
    "#     cap.release()\n",
    "#     print(f\"Total number of frames in the video: {len(frames)}\")\n",
    "    \n",
    "#     return frames\n",
    "\n",
    "# frames = extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22292988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cos_sim_scores(frames):\n",
    "#     similarities = [] \n",
    "\n",
    "#     for i in range(1, len(frames)):\n",
    "#         prev_frame = frames[i - 1].flatten().astype(np.float32)\n",
    "#         curr_frame = frames[i].flatten().astype(np.float32)\n",
    "#         similarity = cosine_similarity([prev_frame], [curr_frame])[0][0]\n",
    "\n",
    "#         similarities.append(similarity) \n",
    "\n",
    "#     return similarities\n",
    "\n",
    "# similarities = cos_sim_scores(frames)\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(similarities, marker='o', label='Cosine Similarity')\n",
    "# plt.title('Cosine Similarity Between Consecutive Frames')\n",
    "# plt.xlabel('Frame Index')\n",
    "# plt.ylabel('Similarity Score')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e243929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segment_scenes(frames, threshold):\n",
    "#     segments = []\n",
    "#     current_segment = [frames[0]]\n",
    "\n",
    "#     for i in range(1, len(frames)):\n",
    "#         prev_frame = frames[i - 1].flatten().astype(np.float32)\n",
    "#         curr_frame = frames[i].flatten().astype(np.float32)\n",
    "#         similarity = cosine_similarity([prev_frame], [curr_frame])[0][0]\n",
    "\n",
    "#         if similarity < threshold:\n",
    "#             segments.append(current_segment)\n",
    "#             current_segment = [frames[i]]\n",
    "#         else:\n",
    "#             current_segment.append(frames[i])\n",
    "\n",
    "#     segments.append(current_segment)\n",
    "#     return segments\n",
    "\n",
    "# threshold=0.965\n",
    "# segments = segment_scenes(frames, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1daed9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_segments = len(segments)\n",
    "# plt.figure(figsize=(10, 4 * num_segments))\n",
    "\n",
    "# for i, segment in enumerate(segments):\n",
    "#     first_frame = segment[0]\n",
    "#     last_frame = segment[-1]\n",
    "\n",
    "#     # Plot first frame\n",
    "#     plt.subplot(num_segments, 2, i * 2 + 1)\n",
    "#     plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "#     plt.title(f'Segment {i+1} - First Frame')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Plot last frame\n",
    "#     plt.subplot(num_segments, 2, i * 2 + 2)\n",
    "#     plt.imshow(cv2.cvtColor(last_frame, cv2.COLOR_BGR2RGB))\n",
    "#     plt.title(f'Segment {i+1} - Last Frame')\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9670c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e48e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize so longer side = 224, then pad shorter side \n",
    "def resize_with_pad(image, size=224):\n",
    "    # Resize so the longer side == size\n",
    "    w, h = image.size\n",
    "    scale = size / max(w, h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    resized = TF.resize(image, (new_h, new_w), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    \n",
    "    # Pad to (size, size)\n",
    "    pad_w = size - new_w\n",
    "    pad_h = size - new_h\n",
    "    padding = (pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2)  # (left, top, right, bottom)\n",
    "    padded = TF.pad(resized, padding, fill=0)  # Fill with black\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3048cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.num_patches = (img_size // patch_size) ** 2\n",
    "#         self.patch_embed = nn.Conv2d(\n",
    "#             in_channels, embed_dim, \n",
    "#             kernel_size=patch_size, \n",
    "#             stride=patch_size\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)  # (batch_size, embed_dim, grid, grid)\n",
    "#         x = x.flatten(2)         # (batch_size, embed_dim, num_patches)\n",
    "#         x = x.transpose(1, 2)    # (batch_size, num_patches, embed_dim)\n",
    "#         return x\n",
    "\n",
    "# class TransformerEncoderBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Self-attention\n",
    "#         x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "#         # Feed Forward\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class TinyVisionTransformer(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=6, num_heads=8):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([\n",
    "#             TransformerEncoderBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "#         ])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, 1 + num_patches, embed_dim)\n",
    "\n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x)\n",
    "#         return x[:, 1:, :]  # discard the cls token, return only patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea90a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate real vision encoder\n",
    "# vision_encoder = TinyVisionTransformer(\n",
    "#     img_size=224,\n",
    "#     patch_size=16,\n",
    "#     in_channels=3,\n",
    "#     embed_dim=768,\n",
    "#     depth=6,\n",
    "#     num_heads=8\n",
    "# )\n",
    "\n",
    "# # Forward\n",
    "# dummy_image = torch.randn(2, 3, 224, 224)  # (batch_size=2)\n",
    "# vision_features = vision_encoder(dummy_image)  # (2, 196, 768)\n",
    "\n",
    "# print(vision_features.shape)  # Should be (batch_size, 14x14=196 patches, 768 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626200db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c49abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", use_fast = True)\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device).eval()\n",
    "\n",
    "# # # --- Step 2: Define a simple RMBL ---\n",
    "# # class SimpleRMBL(torch.nn.Module):\n",
    "# #     def __init__(self, dim=256, num_memory_tokens=4, heads=4):\n",
    "# #         super().__init__()\n",
    "# #         self.memory = torch.nn.Parameter(torch.randn(1, num_memory_tokens, dim))\n",
    "# #         self.transformer = torch.nn.TransformerEncoder(\n",
    "# #             torch.nn.TransformerEncoderLayer(d_model=dim, nhead=heads),\n",
    "# #             num_layers=1\n",
    "# #         )\n",
    "\n",
    "# #     def forward(self, segment_feat, memory=None):\n",
    "# #         if memory is None:\n",
    "# #             memory = self.memory.expand(segment_feat.size(0), -1, -1)\n",
    "# #         combined = torch.cat([memory, segment_feat], dim=1)\n",
    "# #         out = self.transformer(combined)\n",
    "# #         return out[:, :memory.size(1)], out[:, memory.size(1):]\n",
    "\n",
    "\n",
    "# # rmb_layer = SimpleRMBL(dim=256).to(device)\n",
    "# # projector = torch.nn.Linear(2048, 256).to(device) \n",
    "\n",
    "# # # --- Step 4: Segment-Level Description with Memory ---\n",
    "# # def describe_segments_with_memory(segments, initial_prompt):\n",
    "# #     memory = None\n",
    "# #     all_descriptions = []\n",
    "# #     prompt = initial_prompt\n",
    "\n",
    "# #     for idx, frames in enumerate(segments):\n",
    "# #         n = len(frames)\n",
    "# #         frame_indices = [int(n * 0.25), int(n * 0.5), int(n * 0.75)]\n",
    "# #         selected_frames = [frames[i] for i in frame_indices if i < n]\n",
    "# #         segment_description = []\n",
    "\n",
    "# #         for image in selected_frames:\n",
    "# #             inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "# #             outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "# #             description = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# #             segment_description.append(description)\n",
    "# #             segment_description = filter_similar_descriptions(segment_description)\n",
    "\n",
    "# #         all_descriptions.append(\" \".join(segment_description))\n",
    "# #         desc_inputs = processor.tokenizer(segment_description, return_tensors=\"pt\", padding=True).to(device)\n",
    "# #         with torch.no_grad():\n",
    "# #             encoded = model.language_model.get_input_embeddings()(desc_inputs.input_ids) \n",
    "# #             pooled = encoded.mean(dim=1).unsqueeze(1)  \n",
    "# #             pooled_projected = projector(pooled)    \n",
    "\n",
    "# #         memory, _ = rmb_layer(pooled_projected, memory)\n",
    "\n",
    "# #     return all_descriptions, memory\n",
    "\n",
    "# def describe_segments_without_memory(segments, initial_prompt):\n",
    "#     all_descriptions = []\n",
    "#     prompt = initial_prompt\n",
    "\n",
    "#     for idx, frames in enumerate(segments):\n",
    "#         # Randomly select one frame from the segment\n",
    "#         image = random.choice(frames)\n",
    "\n",
    "#         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "#         description = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         all_descriptions.append(description.strip())\n",
    "\n",
    "#     return all_descriptions\n",
    "\n",
    "\n",
    "# # def summarize_video(memory, final_prompt, image):\n",
    "# #     pooled_memory = memory.mean(dim=1)  # (B, 256)\n",
    "\n",
    "# #     # Project memory back to match text embedding dim (approximation)\n",
    "# #     memory_proj = projector.weight.T @ pooled_memory.T  # (2048, B)\n",
    "# #     memory_proj = memory_proj.T  # (B, 2048)\n",
    "\n",
    "# #     inputs = processor(images=image, text=final_prompt, return_tensors=\"pt\").to(device)\n",
    "# #     with torch.no_grad():\n",
    "# #         outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# #     return processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# prompt = \"Describe what is happening in this segment in detail.\"\n",
    "# final_prompt = \"When did the person in the video put the straw into the bottle? Please state your answer with a brief explanation.\"\n",
    "\n",
    "# # descriptions, final_memory = describe_segments_with_memory(segments, prompt)\n",
    "# descriptions = describe_segments_without_memory(segments, prompt)\n",
    "# # summary = summarize_video(final_memory, final_prompt, segments[-1][len(segments[-1]) // 2])\n",
    "\n",
    "# print(\"Segment-level descriptions:\")\n",
    "# for i, d in enumerate(descriptions):\n",
    "#     print(f\"[Segment {i+1}]: {d}\")\n",
    "\n",
    "# # print(\"\\nFinal Summary:\", summary)\n",
    "\n",
    "# context = \" \".join(desc.strip().rstrip(\".\") + \".\" for desc in descriptions)\n",
    "# full_input = f\"This the description of the video: \\n{context} \\n{final_prompt}\"\n",
    "\n",
    "# inputs = processor(text=full_input, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.language_model.generate(**inputs, max_new_tokens=100)\n",
    "# print(processor.tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
