{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb71e51",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f49c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "random.seed(1337)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccbded",
   "metadata": {},
   "source": [
    "# Defining paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"image_folder\"  \n",
    "caption_file = \"captions.txt\"      \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca563a",
   "metadata": {},
   "source": [
    "# Resizing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e48e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize so longer side = 224, then pad shorter side \n",
    "def resize_with_pad(image):\n",
    "    # Resize so the longer side == size\n",
    "    size = 224\n",
    "    w, h = image.size\n",
    "    scale = size / max(w, h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    resized = TF.resize(image, (new_h, new_w), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    \n",
    "    # Pad to (size, size)\n",
    "    pad_w = size - new_w\n",
    "    pad_h = size - new_h\n",
    "    padding = (pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2)  # (left, top, right, bottom)\n",
    "    padded = TF.pad(resized, padding, fill=0)  # Fill with black\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305daf75",
   "metadata": {},
   "source": [
    "# Loading SmolLM2 Model (Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/smollm2\")\n",
    "language_model = AutoModelForSeq2SeqLM.from_pretrained(\"Intel/smollm2\").to(device)\n",
    "for p in language_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2edfa",
   "metadata": {},
   "source": [
    "# Loading ViT Vision Transformer (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1815cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "vision_encoder.eval()\n",
    "for p in vision_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "vision_encoder = vision_encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc965db",
   "metadata": {},
   "source": [
    "# Setting up Q-Former (Visual Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = nn.Parameter(torch.randn(1, 32, 768)).to(device)\n",
    "qformer_blocks = nn.ModuleList([\n",
    "    nn.TransformerEncoderLayer(d_model=768, nhead=8, batch_first=True)\n",
    "    for _ in range(6)\n",
    "]).to(device)\n",
    "qformer_proj = nn.Linear(768, language_model.config.d_model).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(list(qformer_blocks.parameters()) + list(qformer_proj.parameters()) + [query_tokens], lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a450ca6",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qa_file(path):\n",
    "    samples = []\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    img_id = None\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.endswith(\".\"):\n",
    "            if img_id is not None:\n",
    "                for i in range(len(questions)):\n",
    "                    samples.append((img_id, questions[i], answers[i]))\n",
    "            img_id = line.replace(\".\", \"\").strip()\n",
    "            questions = []\n",
    "            answers = []\n",
    "        elif line.startswith(\"Q:\"):\n",
    "            questions.append(line[2:].strip())\n",
    "        elif line.startswith(\"A:\"):\n",
    "            answers.append(line[2:].strip())\n",
    "    if img_id is not None and questions and answers:\n",
    "        for i in range(len(questions)):\n",
    "            samples.append((img_id, questions[i], answers[i]))\n",
    "    return samples\n",
    "\n",
    "samples = parse_qa_file(caption_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(img_id, question, answer):\n",
    "    img_path = os.path.join(image_folder, f\"{img_id}.jpg\")\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = resize_with_pad(image)\n",
    "\n",
    "    q_enc = tokenizer(question, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "    a_enc = tokenizer(answer, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "    return image, q_enc.input_ids[0], q_enc.attention_mask[0], a_enc.input_ids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
